{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiway_Attention_Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZnib7MFopAe"
      },
      "source": [
        "import random\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from torch.optim import Adadelta\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from fastNLP import CrossEntropyLoss\n",
        "from fastNLP.core import Trainer, Tester, AccuracyMetric, Const\n",
        "from fastNLP.core.callback import LRScheduler, EvaluateCallback\n",
        "from fastNLP.embeddings import StaticEmbedding\n",
        "from fastNLP.io.pipe.matching import QuoraPipe\n",
        "\n",
        "import fitlog\n",
        "fitlog.debug()\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import torch as tc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "from fastNLP.core.const import Const\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "import os\n",
        "import math\n",
        "from fastNLP.core.const import Const"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q1n33-nCNV6"
      },
      "source": [
        "import torch as tc\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidrect, dropout):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
        "            batch_first=True, dropout=dropout, bidirectional=bidrect)\n",
        "        self.number = num_layers  \n",
        "        if(bidrect):\n",
        "          self.number *= 2\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        lens = (mask).long().sum(dim=1) #mask : [batch_size, seq_len], x: [batch_size, seq_len, input_size]\n",
        "        lens, idx_sort = tc.sort(lens, descending=True)\n",
        "        _, idx_unsort = tc.sort(idx_sort)\n",
        "        x = x[idx_sort]\n",
        "        x = nn.utils.rnn.pack_padded_sequence(x, lens.cpu(), batch_first=True)\n",
        "        self.rnn.flatten_parameters()\n",
        "        y, h = self.rnn(x)\n",
        "        y, lens = nn.utils.rnn.pad_packed_sequence(y, batch_first=True)\n",
        "        h = h.transpose(0,1).contiguous() #make batch size first\n",
        "        y = y[idx_unsort]      #(batch_size, seq_len, bid * hid_size)\n",
        "        h = h[idx_unsort]          #(batch_size, number, hid_size)\n",
        "        return y, h\n",
        "\n",
        "class Contexualizer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.3):\n",
        "        super(Contexualizer, self).__init__()\n",
        "        self.rnn = RNNModel(input_size, hidden_size, num_layers, True, dropout)\n",
        "        self.output_size = hidden_size*2\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        weights = self.rnn.rnn.all_weights\n",
        "        for w1 in weights:\n",
        "            for w2 in w1:\n",
        "                if len(list(w2.size())) <= 1:\n",
        "                    w2.data.fill_(0)\n",
        "                else: nn.init.xavier_normal_(w2.data, gain=1.414)\n",
        "\n",
        "    def forward(self, s, mask):\n",
        "        y = self.rnn(s, mask)[0]  # (batch_size, seq_len, 2 * hidden_size)\n",
        "        return y\n",
        "\n",
        "class ConcatAttention_Param(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
        "        super(ConcatAttention_Param, self).__init__()\n",
        "        self.ln = nn.Linear(input_size+hidden_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.vq = nn.Parameter(tc.rand(hidden_size))\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.v.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, h, mask):\n",
        "        vq = self.vq.view(1,1,-1).expand(h.size(0), h.size(1), self.vq.size(0)) # h: (batch_size, len, input_size) mask: (batch_size, len)\n",
        "        s = self.v(tc.tanh(self.ln(tc.cat([h,vq],-1)))).squeeze(-1)    \n",
        "        s = s - ((mask.eq(False)).float()*10000)\n",
        "        a = tc.softmax(s,dim=1)\n",
        "        r = a.unsqueeze(-1) * h       # (batch_size, len, input_size)\n",
        "        r = tc.sum(r, dim=1)          # (batch_size, input_size)\n",
        "        return self.drop(r)\n",
        "\n",
        " \n",
        "def get_2dmask(mask_hq, mask_hp, siz=None):\n",
        "    mask_mat = 1\n",
        "    if siz is None:\n",
        "        siz = (mask_hq.size(0), mask_hq.size(1), mask_hp.size(1))\n",
        "    if mask_hp is not None:\n",
        "        mask_mat *= mask_hp.unsqueeze(1).expand(siz)\n",
        "    if mask_hq is not None:\n",
        "        mask_mat *= mask_hq.unsqueeze(2).expand(siz)\n",
        "    return mask_mat\n",
        "\n",
        "def Attention(hq, hp, mask_hq, mask_hp, my_method):\n",
        "    standard_size = (hq.size(0), hq.size(1), hp.size(1), hq.size(-1))\n",
        "    mask_mat = get_2dmask(mask_hq, mask_hp, standard_size[:-1])\n",
        "    hq_mat = hq.unsqueeze(2).expand(standard_size)\n",
        "    hp_mat = hp.unsqueeze(1).expand(standard_size)\n",
        "    s = my_method(hq_mat, hp_mat)           # (batch_size, len_q, len_p)\n",
        "    s = s - ((mask_mat.eq(False)).float() * 10000)\n",
        "    a = tc.softmax(s, dim=1)\n",
        "    q = a.unsqueeze(-1) * hq_mat            #(batch_size, len_q, len_p, input_size)\n",
        "    q = tc.sum(q, dim=1)                    #(batch_size, len_p, input_size)\n",
        "    return q\n",
        "\n",
        "class ConcatAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2, input_size_2=-1):\n",
        "        super(ConcatAttention, self).__init__()\n",
        "        if input_size_2 <= 0:\n",
        "            input_size_2 = input_size\n",
        "        self.ln = nn.Linear(input_size + input_size_2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.v.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "\n",
        "    def my_method(self, hq_mat, hp_mat):\n",
        "        s = tc.cat([hq_mat, hp_mat], dim=-1)\n",
        "        s = self.v(tc.tanh(self.ln(s))).squeeze(-1)    #(batch_size, len_q, len_p)\n",
        "        return s\n",
        "\n",
        "    def forward(self, hq, hp, mask_hq=None, mask_hp=None):\n",
        "        return self.drop(Attention(hq, hp, mask_hq, mask_hp, self.my_method)) #hq: (batch_size, len_q, input_size) mask:(batch_size,len_q)\n",
        "\n",
        "class MinusAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
        "        super(MinusAttention, self).__init__()\n",
        "        self.ln = nn.Linear(input_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.v.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "\n",
        "    def my_method(self, hq_mat, hp_mat):\n",
        "        s = hq_mat - hp_mat\n",
        "        s = self.v(tc.tanh(self.ln(s))).squeeze(-1)    #(batch_size, len_q, len_p) s[j,t]\n",
        "        return s\n",
        "\n",
        "    def forward(self, hq, hp, mask_hq=None, mask_hp=None):\n",
        "        return self.drop(Attention(hq, hp, mask_hq, mask_hp, self.my_method))\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
        "        super(DotProductAttention, self).__init__()\n",
        "        self.ln = nn.Linear(input_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.v.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "\n",
        "    def my_method(self, hq_mat, hp_mat):\n",
        "        s = hq_mat * hp_mat\n",
        "        s = self.v(tc.tanh(self.ln(s))).squeeze(-1)    #(batch_size, len_q, len_p) s[j,t]\n",
        "        return s\n",
        "\n",
        "    def forward(self, hq, hp, mask_hq=None, mask_hp=None):\n",
        "        return self.drop(Attention(hq, hp, mask_hq, mask_hp, self.my_method))\n",
        "\n",
        "class BiLinearAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2, input_size_2=-1):\n",
        "        super(BiLinearAttention, self).__init__()\n",
        "        if input_size_2 <= 0:\n",
        "            input_size_2 = input_size\n",
        "        self.ln = nn.Linear(input_size_2, input_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size  \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "        \n",
        "    def my_method(self, hq, hp, mask_p):\n",
        "        hp = self.ln(hp)\n",
        "        hp = hp * mask_p.unsqueeze(-1)\n",
        "        s = tc.matmul(hq, hp.transpose(-1,-2))\n",
        "\n",
        "        return s\n",
        "\n",
        "    def forward(self, hq, hp, mask_hq=None, mask_hp=None):\n",
        "        standard_size = (hq.size(0), hq.size(1), hp.size(1), hq.size(-1))\n",
        "        mask_mat = get_2dmask(mask_hq, mask_hp, standard_size[:-1])\n",
        "        s = self.my_method(hq, hp, mask_hp)         # (batch_size, len_q, len_p)\n",
        "        s = s - ((mask_mat.eq(False)).float() * 10000)\n",
        "        a = tc.softmax(s, dim=1)\n",
        "        hq_mat = hq.unsqueeze(2).expand(standard_size)\n",
        "        q = a.unsqueeze(-1) * hq_mat                #(batch_size, len_q, len_p, input_size)\n",
        "        q = tc.sum(q, dim=1)                      #(batch_size, len_p, input_size)\n",
        "        return self.drop(q)\n",
        "\n",
        "\n",
        "class AggAttention(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.2):\n",
        "        super(AggAttention, self).__init__()\n",
        "        self.ln = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "        self.vq = nn.Parameter(tc.rand(hidden_size, 1))\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = input_size\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.vq.data)\n",
        "        nn.init.xavier_uniform_(self.v.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln.weight.data)\n",
        "        self.ln.bias.data.fill_(0)\n",
        "        self.vq.data = self.vq.data[:,0]\n",
        "\n",
        "    def forward(self, hs, mask):\n",
        "        hs = tc.cat([h.unsqueeze(0) for h in hs], dim=0) #(4, batch_size, len_q, input_size)\n",
        "        vq = self.vq.view(1,1,1,-1).expand(hs.size(0), hs.size(1), hs.size(2), self.vq.size(0))\n",
        "        s = self.v(tc.tanh(self.ln(tc.cat([hs,vq],-1)))).squeeze(-1) #(4, batch_size, len_q)\n",
        "        s = s - ((mask.unsqueeze(0).eq(False)).float() * 10000)\n",
        "        a = tc.softmax(s, dim=0)\n",
        "        x = a.unsqueeze(-1) * hs\n",
        "        x = tc.sum(x, dim=0) #(batch_size, len_q, input_size)\n",
        "        return self.drop(x)\n",
        "\n",
        "class Aggragator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout=0.3):\n",
        "        super(Aggragator, self).__init__()\n",
        "        now_size = input_size\n",
        "        self.ln = nn.Linear(2 * input_size, 2 * input_size)\n",
        "        now_size = 2 * input_size\n",
        "        self.rnn = Contexualizer(now_size, hidden_size, 2, dropout)\n",
        "        now_size = self.rnn.output_size\n",
        "        self.agg_att = AggAttention(now_size, now_size, dropout)\n",
        "        now_size = self.agg_att.output_size\n",
        "        self.agg_rnn = Contexualizer(now_size, hidden_size, 2, dropout)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.output_size = self.agg_rnn.output_size\n",
        "    def forward(self, qs, hp, mask):\n",
        "        hs = [0 for _ in range(len(qs))] #hp: (batch_size, len_p, input_size)\n",
        "        for i in range(len(qs)):\n",
        "            q = qs[i]\n",
        "            x = tc.cat([q, hp], dim=-1)\n",
        "            g = tc.sigmoid(self.ln(x))\n",
        "            x_star = x*g\n",
        "            h = self.rnn(x_star, mask)\n",
        "            hs[i] = h\n",
        "        \n",
        "        x = self.agg_att(hs, mask)      #(batch_size, len_p, output_size)\n",
        "        h = self.agg_rnn(x, mask)       #(batch_size, len_p, output_size)\n",
        "        return self.drop(h)\n",
        "\n",
        "class Mwan_Imm(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_class=3, dropout=0.2, use_allennlp=False):\n",
        "        super(Mwan_Imm, self).__init__()\n",
        "        self.enc_s1 = Contexualizer(input_size, hidden_size, 2, dropout)\n",
        "        self.enc_s2 = Contexualizer(input_size, hidden_size, 2, dropout)\n",
        "        now_size = self.enc_s1.output_size\n",
        "        self.att_c = ConcatAttention(now_size, hidden_size, dropout)\n",
        "        self.att_b = BiLinearAttention(now_size, hidden_size, dropout)\n",
        "        self.att_m = MinusAttention(now_size, hidden_size, dropout)\n",
        "        self.att_d = DotProductAttention(now_size, hidden_size, dropout)\n",
        "\n",
        "        now_size = self.att_c.output_size\n",
        "        self.agg = Aggragator(now_size, hidden_size, dropout)\n",
        "\n",
        "        now_size = self.enc_s1.output_size\n",
        "        self.pred_1 = ConcatAttention_Param(now_size, hidden_size, dropout)\n",
        "        now_size = self.agg.output_size\n",
        "        self.pred_2 = ConcatAttention(now_size, hidden_size, dropout, input_size_2=self.pred_1.output_size)\n",
        "        now_size = self.pred_2.output_size\n",
        "        self.ln1 = nn.Linear(now_size, hidden_size)\n",
        "        self.ln2 = nn.Linear(hidden_size, num_class)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.ln1.weight.data)\n",
        "        nn.init.xavier_uniform_(self.ln2.weight.data)\n",
        "        self.ln1.bias.data.fill_(0)\n",
        "        self.ln2.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, s1, s2, mas_s1, mas_s2):\n",
        "        hq = self.enc_s1(s1, mas_s1)                #(batch_size, len_q, output_size)\n",
        "        hp = self.enc_s1(s2, mas_s2)\n",
        "        mas_s1 = mas_s1[:,:hq.size(1)]\n",
        "        mas_s2 = mas_s2[:,:hp.size(1)]\n",
        "        mas_q, mas_p = mas_s1, mas_s2\n",
        "        qc = self.att_c(hq, hp, mas_s1, mas_s2)     #(batch_size, len_p, output_size)\n",
        "        qb = self.att_b(hq, hp, mas_s1, mas_s2)\n",
        "        qd = self.att_d(hq, hp, mas_s1, mas_s2)\n",
        "        qm = self.att_m(hq, hp, mas_s1, mas_s2)\n",
        "        ho = self.agg([qc,qb,qd,qm], hp, mas_s2)    #(batch_size, len_p, output_size)\n",
        "        rq = self.pred_1(hq, mas_q)                 #(batch_size, output_size)\n",
        "        rp = self.pred_2(ho, rq.unsqueeze(1), mas_p)#(batch_size, 1, output_size)\n",
        "        rp = rp.squeeze(1)                          #(batch_size, output_size)\n",
        "        rp = F.relu(self.ln1(rp))\n",
        "        rp = self.ln2(rp)\n",
        "        return rp\n",
        "\n",
        "class MwanModel(nn.Module):\n",
        "    def __init__(self, num_class, EmbLayer, args_of_imm={}, ElmoLayer=None):\n",
        "        super(MwanModel, self).__init__()\n",
        "        self.emb = EmbLayer\n",
        "        ElmoLayer = None\n",
        "        self.elmo = None\n",
        "        self.imm = Mwan_Imm(num_class=num_class, **args_of_imm)\n",
        "        self.drop = nn.Dropout(args_of_imm[\"dropout\"])\n",
        "\n",
        "\n",
        "    def forward(self, words1, words2, str_s1=None, str_s2=None, *pargs, **kwargs):\n",
        "        s1, s2 = words1, words2\n",
        "        mas_s1 = (s1 != 0).float()    # mas: (batch_size, seq_len)\n",
        "        mas_s2 = (s2 != 0).float()    # mas: (batch_size, seq_len)\n",
        "        mas_s1.requires_grad = False\n",
        "        mas_s2.requires_grad = False\n",
        "        s1_emb = self.emb(s1)\n",
        "        s2_emb = self.emb(s2)\n",
        "        s1_emb = self.drop(s1_emb)\n",
        "        s2_emb = self.drop(s2_emb)\n",
        "        y = self.imm(s1_emb, s2_emb, mas_s1, mas_s2)\n",
        "        return {Const.OUTPUT: y,}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE4CgCnDCrKI",
        "outputId": "28ada313-1f3a-4da1-906e-15e8dee7b580"
      },
      "source": [
        "import torch\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "print (n_gpu)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = device\n",
        "print (n_gpu)\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "cuda\n",
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_8xZ6FnC-RS",
        "outputId": "7db12e12-7c1b-4569-e11b-27d0c48459db"
      },
      "source": [
        "pd.set_option('display.max_colwidth',1000)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "print (n_gpu)\n",
        "\n",
        "data_bundle = QuoraPipe(lower=True, tokenizer='spacy').process_from_file(paths=\"\")\n",
        "print(len(data_bundle.vocabs[Const.INPUTS(0)]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "16970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoKGnHROABFC",
        "outputId": "016525c8-3fd9-4b93-86dc-6a8629dd2a0b"
      },
      "source": [
        "from fastNLP.embeddings import ElmoEmbedding\n",
        "from fastNLP.embeddings import BertEmbedding\n",
        "model = MwanModel(\n",
        "\tnum_class = len(data_bundle.vocabs[Const.TARGET]),\n",
        "  # EmbLayer = BertEmbedding(data_bundle.vocabs[Const.INPUTS(0)], model_dir_or_name='en-base-cased'),\n",
        "\tEmbLayer = StaticEmbedding(data_bundle.vocabs[Const.INPUTS(0)], requires_grad=False, normalize=False,model_dir_or_name='en-glove-6b-300d'),\n",
        "  # EmbLayer = StaticEmbedding(data_bundle.vocabs[Const.INPUTS(0)], requires_grad=False, normalize=False),\n",
        "  # EmbLayer = ElmoEmbedding(data_bundle.vocabs[Const.INPUTS(0)], model_dir_or_name='en-small', requires_grad=False),\n",
        "\tElmoLayer = None,\n",
        "\targs_of_imm = {\n",
        "\t\t\"input_size\"\t \t: 300 , \n",
        "\t\t\"hidden_size\" \t: 150 , \n",
        "\t\t\"dropout\" \t\t\t: 0.4\t,\n",
        "\t\t\"use_allennlp\" \t\t: False , \n",
        "\t} , \n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 16095 out of 16970 words in the pre-training embedding.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqv6cxQ3HdFj",
        "outputId": "98d20eac-5285-4846-9414-11252ff36eff"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MwanModel(\n",
            "  (emb): StaticEmbedding(\n",
            "    (dropout_layer): Dropout(p=0, inplace=False)\n",
            "    (embedding): Embedding(16796, 300, padding_idx=0)\n",
            "  )\n",
            "  (imm): Mwan_Imm(\n",
            "    (enc_s1): Contexualizer(\n",
            "      (rnn): RNNModel(\n",
            "        (rnn): GRU(300, 150, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "    (enc_s2): Contexualizer(\n",
            "      (rnn): RNNModel(\n",
            "        (rnn): GRU(300, 150, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "      )\n",
            "    )\n",
            "    (att_c): ConcatAttention(\n",
            "      (ln): Linear(in_features=600, out_features=150, bias=True)\n",
            "      (v): Linear(in_features=150, out_features=1, bias=False)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (att_b): BiLinearAttention(\n",
            "      (ln): Linear(in_features=300, out_features=300, bias=True)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (att_m): MinusAttention(\n",
            "      (ln): Linear(in_features=300, out_features=150, bias=True)\n",
            "      (v): Linear(in_features=150, out_features=1, bias=False)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (att_d): DotProductAttention(\n",
            "      (ln): Linear(in_features=300, out_features=150, bias=True)\n",
            "      (v): Linear(in_features=150, out_features=1, bias=False)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (agg): Aggragator(\n",
            "      (ln): Linear(in_features=600, out_features=600, bias=True)\n",
            "      (rnn): Contexualizer(\n",
            "        (rnn): RNNModel(\n",
            "          (rnn): GRU(600, 150, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "        )\n",
            "      )\n",
            "      (agg_att): AggAttention(\n",
            "        (ln): Linear(in_features=600, out_features=300, bias=True)\n",
            "        (v): Linear(in_features=300, out_features=1, bias=False)\n",
            "        (drop): Dropout(p=0.4, inplace=False)\n",
            "      )\n",
            "      (agg_rnn): Contexualizer(\n",
            "        (rnn): RNNModel(\n",
            "          (rnn): GRU(300, 150, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
            "        )\n",
            "      )\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (pred_1): ConcatAttention_Param(\n",
            "      (ln): Linear(in_features=450, out_features=150, bias=True)\n",
            "      (v): Linear(in_features=150, out_features=1, bias=False)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (pred_2): ConcatAttention(\n",
            "      (ln): Linear(in_features=600, out_features=150, bias=True)\n",
            "      (v): Linear(in_features=150, out_features=1, bias=False)\n",
            "      (drop): Dropout(p=0.4, inplace=False)\n",
            "    )\n",
            "    (ln1): Linear(in_features=300, out_features=150, bias=True)\n",
            "    (ln2): Linear(in_features=150, out_features=2, bias=True)\n",
            "  )\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hosIRPQeADGY"
      },
      "source": [
        "optimizer = Adadelta(lr=1, params=model.parameters())\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "callbacks = [\n",
        "    LRScheduler(scheduler),\n",
        "]\n",
        "trainer = Trainer(\n",
        "    train_data       = data_bundle.datasets['train'],\n",
        "    model            = model,\n",
        "    optimizer        = optimizer, \n",
        "    num_workers      = 0,\n",
        "    batch_size       = 32,\n",
        "    save_path        = '/content/sample_data/',\n",
        "    n_epochs         = 12, \n",
        "    print_every      = -1,\n",
        "    dev_data         = data_bundle.datasets['dev'],\n",
        "    metrics          = AccuracyMetric(pred = \"pred\" , target = \"target\"), \n",
        "    metric_key       = 'acc', \n",
        "    device           = [i for i in range(torch.cuda.device_count())],\n",
        "    check_code_level = -1, \n",
        "    callbacks        = callbacks,\n",
        "    loss             = CrossEntropyLoss(pred = \"pred\" , target = \"target\")\n",
        ")\n",
        "trainer.train(load_best_model=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ_FfeKe19-n",
        "outputId": "08de266e-cce9-45e8-c193-910da98dc2b0"
      },
      "source": [
        "#RESULTS"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training epochs started 2020-11-30-15-44-01-719116\n",
            "Evaluate data in 1.93 seconds!\n",
            "Evaluation on dev at Epoch 1/12. Step:146/1752:\n",
            "AccuracyMetric: acc=0.693276\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "\n",
            "Evaluate data in 1.82 seconds!\n",
            "Evaluation on dev at Epoch 2/12. Step:292/1752:\n",
            "AccuracyMetric: acc=0.701231\n",
            "\n",
            "Evaluate data in 1.96 seconds!\n",
            "Evaluation on dev at Epoch 3/12. Step:438/1752:\n",
            "AccuracyMetric: acc=0.704216\n",
            "\n",
            "Evaluate data in 1.8 seconds!\n",
            "Evaluation on dev at Epoch 4/12. Step:584/1752:\n",
            "AccuracyMetric: acc=0.683232\n",
            "\n",
            "Evaluate data in 1.93 seconds!\n",
            "Evaluation on dev at Epoch 5/12. Step:730/1752:\n",
            "AccuracyMetric: acc=0.710021\n",
            "\n",
            "Evaluate data in 1.97 seconds!\n",
            "Evaluation on dev at Epoch 6/12. Step:876/1752:\n",
            "AccuracyMetric: acc=0.712428\n",
            "\n",
            "Evaluate data in 2.07 seconds!\n",
            "Evaluation on dev at Epoch 7/12. Step:1022/1752:\n",
            "AccuracyMetric: acc=0.702156\n",
            "\n",
            "Evaluate data in 1.69 seconds!\n",
            "Evaluation on dev at Epoch 8/12. Step:1168/1752:\n",
            "AccuracyMetric: acc=0.722145\n",
            "\n",
            "Evaluate data in 1.84 seconds!\n",
            "Evaluation on dev at Epoch 9/12. Step:1314/1752:\n",
            "AccuracyMetric: acc=0.725604\n",
            "\n",
            "Evaluate data in =1.93 seconds!\n",
            "Evaluation on dev at Epoch 10/12. Step:1460/1752:\n",
            "AccuracyMetric: acc=0.703242\n",
            "\n",
            "Evaluate data in 1.91 seconds!\n",
            "Evaluation on dev at Epoch 11/12. Step:1606/1752:\n",
            "AccuracyMetric: acc=0.682134\n",
            "\n",
            "Evaluate data in 2.11 seconds!\n",
            "Evaluation on dev at Epoch 12/12. Step:1752/1752:\n",
            "AccuracyMetric: acc=0.692352\n",
            "\n",
            "Reloaded the best model\n",
            "\n",
            "In Epoch:8/12 got best dev performance:\n",
            "AccuracyMetric: acc=0.725604\n",
            "{'best_epoch': 8,\n",
            "'best_eval': {'AccuracyMetric': {'acc': 0.682759}}\n",
            "'best_step': 1168,:\n",
            "'seconds': 292.03}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}